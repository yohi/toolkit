#!/usr/bin/env python3
"""
GitHub PR #104 ã‚³ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‘ã‘ã«æ§‹é€ åŒ–ã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
"""

import json
from datetime import datetime as dt


def process_pr_comments():
    """ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ã—ã¦AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‘ã‘å½¢å¼ã§å‡ºåŠ›"""

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    try:
        with open("pr_104_raw_data.json", encoding="utf-8") as f:
            pr_data = json.load(f)
    except Exception as e:
        print(f"âŒ Error reading pr_data: {e}")
        return

    try:
        with open("pr_104_inline_comments.json", encoding="utf-8") as f:
            inline_comments = json.load(f)
    except Exception as e:
        print(f"âŒ Error reading inline_comments: {e}")
        inline_comments = []

    try:
        with open("pr_104_reviews.json", encoding="utf-8") as f:
            reviews = json.load(f)
    except Exception as e:
        print(f"âŒ Error reading reviews: {e}")
        reviews = []

    # AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‘ã‘ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    structured_data = {
        "metadata": {
            "pull_request_number": pr_data.get("number"),
            "title": pr_data.get("title"),
            "extraction_timestamp": dt.now().isoformat(),
            "total_inline_comments": len(inline_comments),
            "total_reviews": len(reviews),
            "data_sources": ["pr_data", "inline_comments", "reviews"],
            "processing_script": "process_pr_comments.py",
        },
        "pull_request_info": {
            "number": pr_data.get("number"),
            "title": pr_data.get("title"),
            "body": pr_data.get("body", "")[:500]
            + ("..." if len(pr_data.get("body", "")) > 500 else ""),
        },
        "inline_comments": [],
        "review_comments": [],
        "actionable_items": [],
        "coderabbit_analysis": {
            "total_coderabbit_comments": 0,
            "actionable_count": 0,
            "file_coverage": [],
        },
    }

    # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†
    coderabbit_count = 0
    files_mentioned = set()

    for comment in inline_comments:
        user_login = comment.get("user", {}).get("login", "")
        is_coderabbit = "coderabbit" in user_login.lower()

        if is_coderabbit:
            coderabbit_count += 1

        file_path = comment.get("path")
        if file_path:
            files_mentioned.add(file_path)

        structured_comment = {
            "id": comment.get("id"),
            "user": user_login,
            "created_at": comment.get("created_at"),
            "updated_at": comment.get("updated_at"),
            "body": comment.get("body", ""),
            "path": file_path,
            "line": comment.get("line"),
            "start_line": comment.get("start_line"),
            "side": comment.get("side"),
            "position": comment.get("position"),
            "commit_id": comment.get("commit_id"),
            "in_reply_to_id": comment.get("in_reply_to_id"),
            "is_coderabbit": is_coderabbit,
            "body_length": len(comment.get("body", "")),
            "has_suggestions": "```suggestion" in comment.get("body", "").lower(),
        }
        structured_data["inline_comments"].append(structured_comment)

    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†
    for review in reviews:
        user_login = review.get("user", {}).get("login", "")
        is_coderabbit = "coderabbit" in user_login.lower()

        structured_review = {
            "id": review.get("id"),
            "user": user_login,
            "state": review.get("state"),
            "body": review.get("body", ""),
            "submitted_at": review.get("submitted_at"),
            "commit_id": review.get("commit_id"),
            "is_coderabbit": is_coderabbit,
            "body_length": len(review.get("body", "")),
        }
        structured_data["review_comments"].append(structured_review)

    # ã‚¢ã‚¯ã‚·ãƒ§ãƒŠãƒ–ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŠ½å‡ºï¼ˆCodeRabbitã‚³ãƒ¡ãƒ³ãƒˆä¸­å¿ƒï¼‰
    actionable_count = 0
    actionable_keywords = [
        "âš ï¸ Potential issue",
        "ğŸ› ï¸ Refactor suggestion",
        "ğŸ“ Committable suggestion",
        "ğŸ¤– Prompt for AI Agents",
        "Consider",
        "Suggestion:",
        "Issue:",
        "Problem:",
        "Warning:",
        "Error:",
    ]

    for comment in inline_comments:
        user_login = comment.get("user", {}).get("login", "")
        if "coderabbit" in user_login.lower():
            body = comment.get("body", "")

            for keyword in actionable_keywords:
                if keyword.lower() in body.lower():
                    actionable_count += 1
                    structured_data["actionable_items"].append(
                        {
                            "comment_id": comment.get("id"),
                            "type": keyword,
                            "file": comment.get("path"),
                            "line": comment.get("line"),
                            "user": user_login,
                            "created_at": comment.get("created_at"),
                            "content_preview": body[:200] + ("..." if len(body) > 200 else ""),
                            "full_content": body,
                            "priority": (
                                "high"
                                if keyword in ["âš ï¸ Potential issue", "Error:", "Warning:"]
                                else "medium"
                            ),
                        }
                    )
                    break

    # CodeRabbitåˆ†æã‚µãƒãƒªãƒ¼ã‚’æ›´æ–°
    structured_data["coderabbit_analysis"]["total_coderabbit_comments"] = coderabbit_count
    structured_data["coderabbit_analysis"]["actionable_count"] = actionable_count
    structured_data["coderabbit_analysis"]["file_coverage"] = list(files_mentioned)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
    output_file = "pr_104_ai_friendly_comments.json"
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(structured_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… AI-friendly output generated: {output_file}")
        print("ğŸ“Š Summary:")
        print(f"  - Total inline comments: {len(inline_comments)}")
        print(f"  - Total reviews: {len(reviews)}")
        print(f"  - CodeRabbit comments: {coderabbit_count}")
        print(f"  - Actionable items found: {actionable_count}")
        print(f"  - Files with comments: {len(files_mentioned)}")

        # ç°¡æ˜“çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚‚ç”Ÿæˆ
        report_file = "pr_104_analysis_report.md"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(
                f"""# PR #104 ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## åŸºæœ¬æƒ…å ±
- **PRç•ªå·**: {pr_data.get('number')}
- **ã‚¿ã‚¤ãƒˆãƒ«**: {pr_data.get('title')}
- **å‡¦ç†æ—¥æ™‚**: {dt.now().strftime('%Y-%m-%d %H:%M:%S')}

## ã‚³ãƒ¡ãƒ³ãƒˆçµ±è¨ˆ
- **ç·ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆæ•°**: {len(inline_comments)}ä»¶
- **ç·ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°**: {len(reviews)}ä»¶
- **CodeRabbitã‚³ãƒ¡ãƒ³ãƒˆæ•°**: {coderabbit_count}ä»¶
- **ã‚¢ã‚¯ã‚·ãƒ§ãƒŠãƒ–ãƒ«ã‚¢ã‚¤ãƒ†ãƒ æ•°**: {actionable_count}ä»¶
- **è¨€åŠãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {len(files_mentioned)}ä»¶

## ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚³ãƒ¡ãƒ³ãƒˆåˆ†å¸ƒ
"""
            )
            for file_path in sorted(files_mentioned):
                file_comment_count = sum(1 for c in inline_comments if c.get("path") == file_path)
                f.write(f"- `{file_path}`: {file_comment_count}ä»¶\n")

            f.write(
                f"""
## ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
- **æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿**: `{output_file}`
- **å…ƒãƒ‡ãƒ¼ã‚¿**: `pr_104_raw_data.json`, `pr_104_inline_comments.json`, `pr_104_reviews.json`

## AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‘ã‘æƒ…å ±
ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä»¥ä¸‹ã®ç”¨é€”ã§æ´»ç”¨ã§ãã¾ã™ï¼š
1. CodeRabbitã‚³ãƒ¡ãƒ³ãƒˆã®è‡ªå‹•åˆ†æ
2. ã‚¢ã‚¯ã‚·ãƒ§ãƒŠãƒ–ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã®å„ªå…ˆåº¦ä»˜ã‘
3. ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯†åº¦åˆ†æ
4. ä¿®æ­£ææ¡ˆã®è‡ªå‹•ç”Ÿæˆ
"""
            )

        print(f"ğŸ“„ Analysis report generated: {report_file}")

    except Exception as e:
        print(f"âŒ Error writing output files: {e}")


if __name__ == "__main__":
    process_pr_comments()
