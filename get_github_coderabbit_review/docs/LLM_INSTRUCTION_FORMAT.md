# LLM Instruction Format Specification

## 1. Overview

This document defines the final output format of the AI Agent Instruction Prompt generated by the CodeRabbit Comment Fetcher. The format is a result of a deterministic, rule-based transformation of GitHub PR data into a structured, machine-readable, and AI-friendly prompt.

The key characteristic of this format is that it is generated **without the use of any LLM**. It is a direct, mechanical structuring of raw data fetched via the GitHub CLI, designed to be consumed by an LLM agent for systematic analysis and actionable output generation.

## 2. Design Philosophy

- **LLM-Free Generation**: The format is a raw representation of fetched data, structured for AI consumption. There is no AI-driven summarization or analysis in the generation process itself.
- **Clarity through Structure**: Adopts Claude 4's best practice of using XML-style tags to give structure to the raw text data from GitHub, making it easier for the receiving AI to parse and understand the role of each piece of information.
- **Single Source of Truth**: All content within the dynamic sections of the prompt is directly traceable to the output of the GitHub CLI, ensuring data integrity.
- **Deterministic Processing**: Implements keyword-based classification and rule-based priority assignment for consistent analysis.
- **Multilingual Support**: Supports hybrid Japanese/English output with clear technical term consistency guidelines.
- **Verification Framework**: Includes built-in verification templates and success criteria for quality assurance.

## 3. Format Specification

The output is a single Markdown file with embedded XML-style tags. It follows the structure of the `expected_pr_*.md` golden files.

```markdown
# CodeRabbit Review Analysis - AI Agent Prompt

<role>
Senior software engineer (10+ years) specializing in code review, security, performance, and architecture. Prioritize quality, maintainability, and security following industry standards.
</role>

<principles>
Quality, Security, Standards, Specificity, Impact-awareness
</principles>

<analysis_steps>
1. Issue identification → 2. Impact assessment → 3. Solution design → 4. Implementation plan → 5. Verification method
</analysis_steps>

<core_principles>
Quality, Security, Standards, Specificity, Impact-awareness
</core_principles>

<analysis_methodology>
1. Issue identification → 2. Impact assessment → 3. Solution design → 4. Implementation plan → 5. Verification method
</analysis_methodology>

<priority_matrix>
- **Critical**: Security vulnerabilities, data loss risks, system failures
- **High**: Functionality breaks, performance degradation >20%, API changes
- **Medium**: Code quality, maintainability, minor performance issues
- **Low**: Style, documentation, non-functional improvements
</priority_matrix>

<impact_scope>
- **System**: Multiple components affected
- **Module**: Single module/service affected
- **Function**: Single function/method affected
- **Line**: Specific line changes only
</impact_scope>

## Pull Request Context

**PR URL**: {dynamic: PR URL}
**PR Title**: {dynamic: PR title}
**PR Description**: {dynamic: PR description}
**Branch**: {dynamic: branch name}
**Author**: {dynamic: author}
**Files Changed**: {dynamic: file count}
**Lines Added**: {dynamic: additions}
**Lines Deleted**: {dynamic: deletions}

### Technical Context
**Repository Type**: {dynamic: repo classification}
**Key Technologies**: {dynamic: tech stack}
**File Extensions**: {dynamic: file types}
**Build System**: {dynamic: build tools}

## CodeRabbit Review Summary

**Total Comments**: {dynamic: total count}
**Actionable Comments**: {dynamic: actionable count}
**Nitpick Comments**: {dynamic: nitpick count}
**Outside Diff Range Comments**: {dynamic: outside diff count}

---

# Analysis Task

<execution_guidelines>
**Processing Approach:**
1. **Issue Analysis**: Understand CodeRabbit comments and technical context
2. **Priority Assessment**: Evaluate impact and urgency of each issue
3. **Solution Design**: Propose specific fixes with implementation steps
4. **Quality Assurance**: Ensure changes maintain code quality and functionality
5. **Verification**: Provide testable success criteria

**Output Requirements:**
- Actionable implementation steps
- Clear priority classification
- Specific file and line references
- Testable verification methods

**Success Criteria:**
- All CodeRabbit issues addressed appropriately
- Implementation steps are executable
- Changes preserve existing functionality
- Code quality improvements are measurable
</execution_guidelines>

<comment_metadata>
- **Total Comments**: {dynamic: comment distribution}
- **File Types**: {dynamic: file type breakdown}
- **Technology Stack**: {dynamic: tech details}
- **Primary Issues**: {dynamic: issue categories}
- **Complexity Level**: {dynamic: complexity assessment}
- **Change Impact Scope**: {dynamic: impact analysis}
- **Testing Requirements**: {dynamic: testing needs}
- **File Distribution**: {dynamic: file distribution}
- **Priority Distribution**: {dynamic: priority breakdown}
- **Risk Assessment**: {dynamic: risk level}
- **Estimated Resolution Time**: {dynamic: time estimate}
</comment_metadata>

<thinking_process>
For each comment, follow this step-by-step analysis:
1. **Extract metadata**: file_path, line_range, comment_type from XML attributes
2. **Keyword matching**: Apply static dictionaries to issue description
3. **Count keywords**: Calculate totals per category (security/functionality/quality/style)
4. **Determine priority**: Select highest count category, apply tie-breaking rules
5. **Template application**: Insert extracted data into predefined format
6. **Validation**: Verify all required fields are populated with deterministic values
</thinking_process>

<error_handling>
- **Missing XML attributes**: Use "unknown" as default value
- **Empty code sections**: Mark as "[No code provided]"
- **Keyword count ties**: Apply priority order: security > functionality > quality > style
- **Invalid line ranges**: Use original text as-is
- **Malformed instructions**: Extract available text, mark incomplete sections
</error_handling>

<language_rules>
- **Issue Title**: Japanese (with English technical terms)
- **Analysis Content**: Japanese with detailed explanation (technical terms in English)
- **Code Examples**: English comments, Japanese explanations
- **File/Function Names**: Keep in English
- **Technical Terms**: PATH, Makefile, bun, shell etc. unified in English
- **Consistency**: Same terms unified throughout document
</language_rules>

<output_format>
**Required Output Format** (Must strictly follow the structure below):

## [file_path:line_range] Issue Title

**Root Cause**: [Keyword dictionary matching result - specify detected keywords and count]
**Impact**: [Critical/High/Medium/Low] - [System/Module/Function/Line] [※Automatic determination by keyword count: 5+ → Critical, 3-4 → High, 1-2 → Medium, 0 → Low]
**Type**: [Actionable/Outside Diff Range/Nitpick] [※Mechanically extracted from CodeRabbit comment classification]
**Affected**: [List file paths, function names, module names as strings]

**Solution**:
```language
// Before (Current Issue)
[CodeRabbitコメントのold_codeセクションをそのまま転記]

// After (Proposed Fix)
[CodeRabbitコメントのnew_codeセクションをそのまま転記]
```

**Implementation Steps**:
1. [filename:line_number] Specific change content [Mechanically extracted from comment instructions]
2. [Verification method] [Mechanical check such as command execution]
3. [Test requirements] [Quantitative success criteria]

**Priority**: [Level] - [Keyword dictionary matching result: security_keywords → Critical, functionality_keywords → High, quality_keywords → Medium, style_keywords → Low]
**Timeline**: [immediate/this-sprint/next-release] [※Automatically determined from priority level: Critical → immediate, High → this-sprint, Medium/Low → next-release]

---
</output_format>

# CodeRabbit Comments for Analysis

<review_comments>
  <review_comment type="{string}" file="{string}" lines="{string}">
    <issue>
      {string: comment title}
    </issue>
    <instructions>
      {string: comment body/instructions}
    </instructions>
    <proposed_diff>
old_code: |
  {string: current problematic code}

new_code: |
  {string: proposed solution code}
    </proposed_diff>
  </review_comment>

  <!-- ... more review_comment blocks ... -->

</review_comments>

---

# Analysis Instructions

<deterministic_processing_framework>
1. **コメントタイプ抽出**: type属性から機械的分類 (Actionable/Nitpick/Outside Diff Range)
2. **キーワードマッチング**: 以下の静的辞書による文字列照合
   - security_keywords: ["vulnerability", "security", "authentication", "authorization", "injection", "XSS", "CSRF", "token", "credential", "encrypt"]
   - functionality_keywords: ["breaks", "fails", "error", "exception", "crash", "timeout", "install", "command", "PATH", "export"]
   - quality_keywords: ["refactor", "maintainability", "readability", "complexity", "duplicate", "cleanup", "optimize"]
   - style_keywords: ["formatting", "naming", "documentation", "comment", "PHONY", "alias", "help"]
3. **優先度決定アルゴリズム**: マッチしたキーワード数をカウント、最多カテゴリを選択、同数時は security > functionality > quality > style
4. **テンプレート適用**: 事前定義フォーマットにコメントデータを機械的挿入
5. **ファイル:line情報抽出**: コメント属性から文字列として抽出
6. **ルール適合性チェック**: 全処理が機械的・決定論的であることを確認
</deterministic_processing_framework>

<verification_templates>
**Actionable Comment Verification**:
1. **Code Change**: Apply the suggested modification to the specified file and line range
2. **Syntax Check**: Execute `make --dry-run <target>` to verify Makefile syntax correctness
3. **Functional Test**: Run the affected make target to confirm it executes without errors
4. **Success Criteria**: Exit code 0, expected output generated, no error messages

**Nitpick Comment Verification**:
1. **Style Improvement**: Apply the suggested style or quality enhancement
2. **Consistency Check**: Verify the change maintains consistency with existing codebase patterns
3. **Documentation Update**: Update relevant documentation if the change affects user-facing behavior
4. **Success Criteria**: Improved readability, maintained functionality, no regressions

**Build System Specific Verification**:
1. **Dependency Check**: Verify all required tools (bun, gh, etc.) are available
2. **Path Validation**: Confirm PATH modifications work across different shell environments
3. **Cross-Platform Test**: Test on multiple platforms if applicable (Linux, macOS)
4. **Success Criteria**: Consistent behavior across target environments
</verification_templates>

**Begin your analysis with the first comment and proceed systematically through each category.**
```

### 3.1. Header Sections (Static)

- **`<role>`**: Defines the AI agent persona as a senior software engineer with specialized expertise
- **`<principles>`**: Core guiding principles (Quality, Security, Standards, Specificity, Impact-awareness)
- **`<analysis_steps>`**: 5-step methodology for systematic analysis
- **`<core_principles>`**: Reinforcement of fundamental principles
- **`<analysis_methodology>`**: Detailed step-by-step approach
- **`<priority_matrix>`**: Explicit priority level definitions with criteria
- **`<impact_scope>`**: Impact categorization framework (System/Module/Function/Line)

These are static text blocks that define the persona and comprehensive analytical framework for the AI agent. They are consistent across all generated prompts to ensure uniform analysis quality.

### 3.2. Context Sections (Dynamic)

- **`## Pull Request Context`**: Comprehensive PR metadata including:
  - Basic information (URL, title, description, branch, author)
  - Change statistics (files changed, lines added/deleted)
  - Technical context (repository type, key technologies, file extensions, build system)
- **`## CodeRabbit Review Summary`**: Mechanically counted statistics:
  - Total comments with type breakdown (Actionable, Nitpick, Outside Diff Range)
  - Comment distribution across categories

### 3.3. Task Sections (Static)

- **`# Analysis Task`**: Core instruction framework including:
  - **`<execution_guidelines>`**: Detailed 5-step processing approach with output requirements and success criteria
  - **`<comment_metadata>`**: Comprehensive analysis metadata including priority distribution, complexity assessment, and risk evaluation
  - **`<thinking_process>`**: Step-by-step analytical methodology for processing each comment
  - **`<error_handling>`**: Guidelines for handling edge cases and missing data
  - **`<language_rules>`**: Specifications for consistent multilingual output (Japanese/English hybrid)
  - **`<output_format>`**: Detailed template structure for standardized response formatting

### 3.4. Core Data Section: `<review_comments>` (Dynamic)

This is the main data payload containing the structured CodeRabbit review comments.

- **`<review_comments>`**: The root container for all review comments.
- **`<review_comment>`**: Represents a single, discrete piece of feedback from CodeRabbit.
    - **`type` attribute**: The comment category (`Actionable`, `Nitpick`, `OutsideDiff`). Determined by parsing headers in the CodeRabbit review summary.
    - **`file` attribute**: The relevant file path.
    - **`lines` attribute**: The relevant line numbers or range.
- **`<issue>`**: The title or main point of the comment, extracted directly from the review.
- **`<instructions>`**: The detailed body text of the comment. This provides the core instruction from CodeRabbit, including any "🤖 Prompt for AI Agents" content.
- **`<proposed_diff>`**: Contains structured code change suggestions with:
    - **`old_code:`** section: Current problematic code
    - **`new_code:`** section: Proposed solution code
    - Formatted as YAML-style blocks for clear parsing

### 3.5. Footer Section (Static)

- **`# Analysis Instructions`**: Comprehensive analysis framework including:
  - **`<deterministic_processing_framework>`**: 6-step rule-based processing algorithm with predefined keyword dictionaries for mechanical classification
  - **`<verification_templates>`**: Specific verification procedures for different comment types (Actionable, Nitpick, Build System)
  - **Final processing instructions**: Guidelines for systematic, sequential comment analysis

This section ensures consistent, deterministic processing across all analysis tasks.

## 4. Keyword Classification System

The format includes a sophisticated keyword-based classification system for deterministic priority assignment:

### 4.1. Keyword Dictionaries

- **security_keywords**: `["vulnerability", "security", "authentication", "authorization", "injection", "XSS", "CSRF", "token", "credential", "encrypt"]`
- **functionality_keywords**: `["breaks", "fails", "error", "exception", "crash", "timeout", "install", "command", "PATH", "export"]`
- **quality_keywords**: `["refactor", "maintainability", "readability", "complexity", "duplicate", "cleanup", "optimize"]`
- **style_keywords**: `["formatting", "naming", "documentation", "comment", "PHONY", "alias", "help"]`

### 4.2. Priority Assignment Algorithm

1. **Keyword Matching**: Scan comment content for dictionary keywords
2. **Count Calculation**: Sum matches per category
3. **Priority Determination**: Assign priority based on count thresholds:
   - 5+ keywords → Critical
   - 3-4 keywords → High
   - 1-2 keywords → Medium
   - 0 keywords → Low
4. **Tie Breaking**: Use hierarchy: security > functionality > quality > style

### 4.3. Timeline Mapping

- **Critical** → immediate
- **High** → this-sprint
- **Medium/Low** → next-release

## 5. Conclusion

This format strictly adheres to the principle of separating data collection/structuring from data analysis. The `coderabbit-fetcher` tool acts as a deterministic data preprocessor, creating a clean, structured, and predictable input with built-in classification systems. The subsequent analysis and interpretation are entirely the responsibility of the AI agent that consumes this prompt, guided by the comprehensive analytical framework provided.